{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613fbb0c",
   "metadata": {},
   "source": [
    "**DSS : BUILDING LARGE LANGUAGE MODELS FOR BUSINESS APPLICATIONS Day 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f355a3be",
   "metadata": {},
   "source": [
    "# Creating Conservational AI with Large Language Models for Business\n",
    "\n",
    "## Training Objective\n",
    "\n",
    "In this module, we will embark on a journey to explore the fascinating world of Conversational AI and its applications in various business domains. We will delve into the principles, techniques, and best practices for harnessing the potential of LLMs to build robust and effective conversational systems. Whether you are a business professional, data scientist, or developer, this book will equip you with the knowledge and skills needed to leverage LLMs for creating advanced conversational AI solutions tailored to the specific needs of your organization.\n",
    "\n",
    "- **Large Language Models: Architecture, Transformer, and Key Concepts**\n",
    "   - Overview of Large Language Models and their Architecture\n",
    "   - Understanding what is Transformer\n",
    "   - Explanation of pre-training and fine-tuning of language models\n",
    "   - Introduction to popular Large Language Models like GPT-3, GPT-2, and BERT\n",
    "   - Understanding the capabilities and limitations of Large Language Models\n",
    "   - Explanation of the LangChain concept\n",
    "   - Setting the API key and .env\n",
    "\n",
    "\n",
    "- **Building Question-Answering Systems with Large Language Models**\n",
    "   - Introduction to Question-Answering System\n",
    "   - Steps involved in connecting databases with LLM\n",
    "   - Basics of building a Question-Answering System using LLM with a database\n",
    "   - Demonstration of using OpenAI and LangChain to build a Question-Answering System\n",
    "   - Using LangChain and OpenAI to build a Question-Answering System with text data\n",
    "   - Steps involved in connecting CSV data with LLM\n",
    "   - Demonstration of using LangChain and OpenAI to build a Question-Answering System with text data\n",
    "\n",
    "\n",
    "- **Text Generation with HuggingFace**\n",
    "   - Introduction to the Text Generation model in HuggingFace\n",
    "   - Setting the .env token key\n",
    "   - Applying HuggingFace's Inference API to use LLM without OpenAI credits\n",
    "   - Integrating HuggingFace's Inference API into the previously built Question-Answering System\n",
    "   - Demonstration of using HuggingFace's Inference API to build a Question-Answering System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e8e696",
   "metadata": {},
   "source": [
    "# Large Language Models\n",
    "\n",
    "\n",
    "**What is Large Language Models?**\n",
    "\n",
    "Large Language Models (LLMs) is an advanced type of language model that represent a breakthrough in the field of natural language processing (NLP). These models are designed to understand and generate human-like text by leveraging the power of deep learning algorithms and massive amounts of data.\n",
    "\n",
    "If you've ever chatted with a virtual assistant or interacted with an AI customer service agent, you might have interacted with a large language model without even realizing it. These models have a wide range of applications, from chatbots to language translation to content creation.\n",
    "\n",
    "Some of the most impressive large language models are developed by OpenAI. Their GPT-3 model, for example, has over [175 billion parameters](https://www.techtarget.com/searchenterpriseai/definition/GPT-3#:~:text=GPT%2D3%20has%20more%20than,(BERT)%20and%20Turing%20NLG.) and is able to perform tasks like [summarization](https://wandb.ai/mostafaibrahim17/ml-articles/reports/Compressing-the-Story-The-Magic-of-Text-Summarization--VmlldzozNTYxMjc2), [question-answering](https://wandb.ai/mostafaibrahim17/ml-articles/reports/The-Answer-Key-Unlocking-the-Potential-of-Question-Answering-With-NLP--VmlldzozNTcxMDE3), and even creative writing.\n",
    "\n",
    "\n",
    "\n",
    "**How a Large Language Model was Built?**\n",
    "\n",
    "The architecture of LLMs is based on the Transformer model, which has revolutionized NLP tasks. The Transformer model utilizes a self-attention mechanism that allows the model to focus on different parts of the input sequence, capturing dependencies and relationships between words more effectively. This architecture enables LLMs to generate coherent and contextually relevant responses, making them valuable tools for a wide range of applications.\n",
    "\n",
    "A large-scale transformer model known as a “large language model” is typically too massive to run on a single computer and is, therefore, provided as a service over an API or web interface. These models are trained on vast amounts of text data from sources such as books, articles, websites, and numerous other forms of written content. By analyzing the statistical relationships between words, phrases, and sentences through this training process, the models can generate coherent and contextually relevant responses to prompts or queries.\n",
    "\n",
    "*ChatGPT’s GPT-3* model, for instance, was trained on massive amounts of internet text data, giving it the ability to understand various languages and possess knowledge of diverse topics. As a result, it can produce text in multiple styles. While its capabilities may seem impressive, including translation, text summarization, and question-answering, they are not surprising, given that these functions operate using special “grammars” that match up with prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b186b6",
   "metadata": {},
   "source": [
    "### Understanding what is Transformer\n",
    "\n",
    "The Transformer is a type of deep learning architecture that has revolutionized the field of natural language processing. It was introduced in the paper [\"Attention Is All You Need\" by Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762). The Transformer model employs self-attention mechanisms to capture dependencies between words in a sentence, enabling it to learn contextual relationships and generate coherent and contextually relevant text.\n",
    "\n",
    "The Transformer architecture excels at handling text data which is inherently sequential. They take a text sequence as input and produce another text sequence as output. eg. to translate an input French sentence to English.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/t/the_transformer_3.png\">\n",
    "\n",
    "The Transformer architecture consists of two main components: the encoder and the decoder. The encoder processes the input sequence and generates a representation, which is then passed to the decoder. The decoder generates the output sequence based on the encoder's representation and previous outputs.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png\">\n",
    "\n",
    "Here are the key components and concepts of the Transformer architecture:\n",
    "\n",
    "1. **Positional Encoding**: Transformers incorporate positional encoding to provide the model with information about the order of words in the input sequence. Positional encoding is usually added to the input embeddings and allows the model to differentiate between the positions of words.\n",
    "\n",
    "![positional_encoding](assets/positional_encoding.png)\n",
    "\n",
    "2. **Self-Attention Mechanism**: Self-attention allows each word in the input sequence to attend to all other words. It computes the attention weight between each pair of words and uses them to generate a weighted sum of the word embeddings. This mechanism enables the model to capture long-range dependencies and learn contextual relationships effectively.\n",
    "\n",
    "![self_attention](assets/self_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e015e2a",
   "metadata": {},
   "source": [
    "### Pre-training and Fine-tuning of Language Models\n",
    "\n",
    "Pre-training and fine-tuning are two key steps in the training process of language models, including Large Language Models (LLMs). \n",
    "\n",
    "**Pre-training**: In the pre-training phase, a language model is trained on a large corpus of unlabeled text data. During this phase, the model learns to predict missing words in sentences based on the surrounding context. It develops an understanding of language patterns, grammar, and contextual relationships. The pre-training process typically involves techniques like masked language modeling, where certain words are randomly masked and the model learns to predict them based on the remaining context.\n",
    "\n",
    "**Fine-tuning**: After pre-training, the language model is fine-tuned on specific labeled datasets for specific downstream tasks. Fine-tuning involves training the pre-trained model on labeled data related to a particular task, such as question answering, sentiment analysis, or text classification. This process allows the model to adapt to the specific task by learning task-specific patterns and improving its performance. Fine-tuning is performed on a smaller dataset, which is typically task-specific and labeled by human experts.\n",
    "\n",
    "In the context of Large Language Models (LLMs), the terms \"pre-trained\" and \"fine-tuned\" refer to two stages in the model development process. This two-step process offers several advantages:\n",
    "\n",
    "- Pre-training on large-scale data helps LLMs learn from a diverse range of linguistic patterns and structures, enhancing their language understanding capabilities.\n",
    "- Fine-tuning allows LLMs to adapt to specific tasks or domains, improving their performance and making them more efficient in generating desired outputs.\n",
    "- Fine-tuning requires comparatively less labeled data than training from scratch, making it a practical approach when labeled data is limited.\n",
    "\n",
    "LLMs, such as GPT-3, GPT-2, and BERT, are examples of large-scale language models that have undergone extensive pre-training and fine-tuning processes. They have been trained on vast amounts of text data and have a large number of parameters. This pre-training and fine-tuning approach allows LLMs to capture complex language patterns, generate coherent text, and perform well on a wide range of natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1321c58",
   "metadata": {},
   "source": [
    "### Popular Large Language Models \n",
    "\n",
    "Popular Large Language Models (LLMs) are advanced models that have gained significant attention in the field of natural language processing. They have been trained on massive amounts of text data and have a large number of parameters, allowing them to capture complex language patterns and generate coherent text.\n",
    "\n",
    "Here are some examples of popular LLMs:\n",
    "\n",
    "1. **[GPT-3](https://openai.com/blog/gpt-3-apps) (Generative Pre-trained Transformer 3)**: GPT-3 is a state-of-the-art language model developed by OpenAI. It is renowned for its impressive size, consisting of 175 billion parameters. GPT-3 has been trained on a vast amount of internet text data, enabling it to understand and generate human-like text. It can perform a wide range of natural language processing tasks, including language translation, text completion, sentiment analysis, and more. GPT-3 has shown remarkable capabilities in generating coherent and contextually relevant responses, making it a powerful tool for various applications.\n",
    "\n",
    "2. **[GPT-2](https://huggingface.co/gpt2) (Generative Pre-trained Transformer 2)**: GPT-2 is the predecessor to GPT-3, also developed by OpenAI. Although smaller in size with 1.5 billion parameters, GPT-2 still delivers impressive language generation capabilities. It has been trained on diverse internet text sources, allowing it to produce high-quality text in a variety of styles and topics. GPT-2 is widely used for tasks such as text completion, text generation, and language understanding.\n",
    "\n",
    "3. **[BERT](https://machinelearningmastery.com/a-brief-introduction-to-bert/) (Bidirectional Encoder Representations from Transformers)**: BERT is a groundbreaking language model developed by Google. It introduced the concept of bidirectional training, which significantly improved the understanding of context in natural language processing. BERT has been trained on large-scale text data and employs a transformer architecture. It excels in various language understanding tasks, including question-answering, sentiment analysis, named entity recognition, and more. BERT has set new benchmarks in several natural language processing tasks and has been widely adopted in both research and industry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9675a5",
   "metadata": {},
   "source": [
    "### Capabilities and limitations of Large Language Models\n",
    "\n",
    "Large language models like GPT-3, GPT-2, and BERT exhibit impressive capabilities in tasks such as :\n",
    "\n",
    "- text generation, \n",
    "- language translation, \n",
    "- text understanding\n",
    "- sentiment analysis,\n",
    "- text summarization\n",
    "- question answering,\n",
    "- etc.\n",
    "\n",
    "They can understand complex language structures, generate coherent text, and perform well on a range of natural language processing tasks. \n",
    "\n",
    "However, it's essential to acknowledge the limitations of LLMs:\n",
    "\n",
    "- Bias and Ethical Concerns: LLMs can inherit biases present in the training data, leading to biased or controversial outputs. Ensuring fairness, diversity, and ethical use of LLMs is a critical challenge.\n",
    "- Contextual Understanding: While LLMs can generate coherent text, they may sometimes struggle with understanding the broader context or resolving ambiguous statements.\n",
    "- Lack of Real-World Knowledge: LLMs are trained on vast amounts of text data, but they lack true real-world experience and common-sense reasoning abilities. They may provide accurate information but lack true understanding.\n",
    "- Computational Requirements: LLMs are computationally intensive, requiring significant computational resources for training and inference. This can limit their accessibility and scalability for some applications.\n",
    "- Data Dependency: LLMs heavily rely on the quality and diversity of the training data. Inadequate or biased data can impact their performance and generalization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46141978",
   "metadata": {},
   "source": [
    "## Introduction to LangChain\n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a framework for developing applications powered by language models that refers to the integration of multiple language models and APIs to create a powerful and flexible language processing pipeline. It involves connecting different language models, such as OpenAI's GPT-3 or GPT-2, with other tools and APIs to enhance their functionality and address specific business needs. \n",
    "\n",
    "The LangChain concept aims to leverage the strengths of each language model and API to create a comprehensive language processing system. It allows developers to combine different models for tasks like question answering, text generation, translation, summarization, sentiment analysis, and more.\n",
    "\n",
    "The core idea of the library is that we can **“chain”**“ together different components to create more advanced use cases around LLMs. Chains may consist of multiple components from several modules:\n",
    "\n",
    "1. **Prompt templates**: Prompt templates are templates for different types of prompts. Like “chatbot” style templates, ELI5 question-answering, etc\n",
    "\n",
    "2. **LLMs**: Large language models like GPT-3, BLOOM, etc\n",
    "\n",
    "3. **Agents**: Agents use LLMs to decide what actions should be taken. Tools like web search or calculators can be used, and all are packaged into a logical loop of operations.\n",
    "\n",
    "4. **Memory**: Short-term memory, long-term memory.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8274f27e",
   "metadata": {},
   "source": [
    "### Environment Set-up\n",
    "\n",
    "Using LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs.\n",
    "\n",
    "#### Setting API key and `.env`\n",
    "\n",
    "Accessing the API requires an API key, which you can get by creating an account and heading here. When setting up an API key and using a .env file in your Python project, you follow these general steps:\n",
    "\n",
    "1. **Obtain an API key**: If you're working with an external API or service that requires an API key, you need to obtain one from the provider. This usually involves signing up for an account and generating an API key specific to your project.\n",
    "\n",
    "2. **Create a .env file**: In your project directory, create a new file and name it \".env\". This file will store your API key and other sensitive information securely.\n",
    "\n",
    "3. **Store API key in .env**: Open the .env file in a text editor and add a line to store your API key. The format should be `API_KEY=your_api_key`, where \"API_KEY\" is the name of the variable and \"your_api_key\" is the actual value of your API key. Make sure not to include any quotes or spaces around the value.\n",
    "\n",
    "4. **Load environment variables**: In your Python code, you need to load the environment variables from the .env file before accessing them. Import the dotenv module and add the following code at the beginning of your script:\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "```\n",
    "\n",
    "> `dotenv` library is a popular Python library that simplifies the process of loading environment variables from a .env file into your Python application. It allows you to store configuration variables separately from your code, making it easier to manage sensitive information such as API keys, database credentials, or other environment-specific settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e2f8c5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c5c67",
   "metadata": {},
   "source": [
    "## LangChain Quickstart\n",
    "\n",
    "In `LangChain`, a QuickStart involves working with three key components: Prompt, Chain, and Agent. \n",
    "\n",
    "With the **Prompt, Chain, and Agent** components working together, we can engage in interactive conversations with the language model. The Prompt sets the context or initiates the conversation, the Chain maintains the conversation history, and the Agent manages the communication between the user and the language model.\n",
    "\n",
    "Using these components, we can build dynamic and **interactive applications** that involve back-and-forth interactions with the language model, allowing we to create **conversational agents**, **chatbots**, **question-answering systems**, and more.\n",
    "\n",
    "To interact `LangChain` library with an OpenAI language model, we should:\n",
    "\n",
    "1. **Importing the Required Module**: The code imports the LangChain library by using the statement `from langchain import OpenAI`.\n",
    "\n",
    "2. **Creating an OpenAI Instance**: The code creates an instance of the `OpenAI` class and assigns it to the variable `llm`. This instance represents the connection to the OpenAI language model.\n",
    "\n",
    "3. **Setting the Temperature Parameter**: The `temperature` parameter is passed to the `OpenAI` instance during its initialization. Temperature is a parameter that controls the randomness of the language model's output. \n",
    "> A higher temperature value (e.g., 0.9) makes the generated text more **diverse and creative**, while a lower value (e.g., 0.2) makes it more **focused and deterministic**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e4f04ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.1, ) #gpt3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42c580",
   "metadata": {},
   "source": [
    "By creating an instance of `OpenAI` and setting the desired temperature, we can now use the `llm` object to interact with the OpenAI language model. We can pass prompts or messages to the `llm` object, receive the generated responses, and customize the behavior of the language model using additional parameters and methods provided by the LangChain library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a6abc7",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "\n",
    "#### Basic Prompt\n",
    "\n",
    "**Prompt** refers to the initial input or instruction given to the language model to generate a response. It sets the context and provides guidance for the language model to produce relevant and coherent text. \n",
    "\n",
    "In this example, the prompt asks for a suggestion of a good name for a brand specializing in local burgers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d8e8f41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Burger Towne.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is a good name for a brand that makes local burger?\"\n",
    "print(llm.predict(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7306d7",
   "metadata": {},
   "source": [
    "The language model then uses its knowledge and training to generate a response that fits the given prompt. Notice every re-run it generate new answer.\n",
    "\n",
    "> The `llm.predict()` function is called with the prompt as the input. This function sends the prompt to the language model and **generates** a response based on the given input. The generated text represents the **language model's prediction** or **completion** of the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa86abd",
   "metadata": {},
   "source": [
    "We can also did it in other languages, let's try with Bahasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d6fcab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Delicious Delightful's Pisang Goreng Mentai.\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(\"Nama yang bagus untuk brand yang membuat pisang goreng mentai?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6edfc99",
   "metadata": {},
   "source": [
    "By simply providing a prompt in Bahasa (Indonesian language), we can obtain a generated text response in Bahasa as well. This showcases the versatility of language models like LangChain in understanding and generating text in various languages, allowing for multilingual applications and interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad4af7a",
   "metadata": {},
   "source": [
    "#### Prompt Templates\n",
    "\n",
    "LLM applications typically utilize a prompt template instead of directly inputting user queries into the LLM. This approach involves incorporating the user input into a larger text context known as a prompt template.\n",
    "\n",
    "A **prompt template** is a structured format designed to generate prompts in a consistent manner. It consists of a text string, referred to as the \"template,\" which can incorporate various parameters provided by the end user to create a dynamic prompt.\n",
    "\n",
    "The prompt template can include:\n",
    "\n",
    "- Instructions to guide the language model's response.\n",
    "- A set of few-shot examples to assist the language model in generating more accurate and contextually appropriate outputs.\n",
    "- A question posed to the language model.\n",
    "\n",
    "In the previous example, the text passed to the model contained instructions to generate a brand name based on a given description. In our application, it would be convenient for users to only provide the description of their company or product without the need to explicitly provide instructions to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f742fa",
   "metadata": {},
   "source": [
    "To create a prompt template using LangChain, we begin by importing the `PromptTemplate` class from the `langchain.prompts` module. This class allows us to create and manipulate prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e34f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7549a224",
   "metadata": {},
   "source": [
    "**Create** a prompt template: Use the `PromptTemplate.from_template()` method to create a `PromptTemplate` object from the template string. \n",
    "\n",
    "In this case, the template string is \"What is a good name for a brand that makes {product}?\", where `{product}` acts as a placeholder for the product name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ed97804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "template_prompt = PromptTemplate.from_template(\"What is a good name for a brand that makes {rumah}?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56b8997",
   "metadata": {},
   "source": [
    "**Format** the prompt template: Use the `.format()` method of the `PromptTemplate` object to replace the placeholder in the template with the desired value. In this case, the placeholder `{product}` is replaced with the string \"local burger\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb9b3098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a brand that makes local burger?\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt template\n",
    "prompt = template_prompt.format(rumah=\"local burger\")\n",
    "\n",
    "# Print the prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cb243a",
   "metadata": {},
   "source": [
    "Notice the instruction changes automatically based on user input, this instruction will be input to `llm` to generate the response. Let's get the response generated by the language model (`llm`) based on the given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58cf787f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Burger Boro.\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c3e92",
   "metadata": {},
   "source": [
    "Because this is a template, it can handle more than one input, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5f26af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['adjective', 'subject'] output_parser=None partial_variables={} template='Write a {adjective} poem about {subject}' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "# defines a string template for a poem\n",
    "template = \"Write a {adjective} poem about {subject}\"\n",
    "\n",
    "# creates a prompt template\n",
    "poem_template = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"subject\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "print(poem_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d081c7de",
   "metadata": {},
   "source": [
    "Formats the template by replacing the placeholders `{adjective}` and `{subject}` with the provided values. The resulting string will be \"Write a sad poem about ducks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "62120dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a sad poem about ducks'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_template.format(adjective='sad', subject='ducks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e74787c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The little ducks waddle on by,\n",
      "Hoping for some company,\n",
      "But sadly no one stops,\n",
      "To offer them some kindness\n",
      "\n",
      "The power of the wind ripples along,\n",
      "Their hearts and bodies heavy and worn,\n",
      "Lonely days on the lake,\n",
      "No one stopping to take\n",
      "\n",
      "Just quack after quack,\n",
      "Resounding like a sad lack,\n",
      "Of something they desire,\n",
      "A place in this world to thrive\n",
      "\n",
      "The Mallards take flight,\n",
      "But this is their plight,\n",
      "In the sky looking down,\n",
      "At the pond so still and sound\n",
      "\n",
      "The ducks mourn and cry,\n",
      "For the life they could have had,\n",
      "As they look up to the sky,\n",
      "Unable to comprehend why.\n"
     ]
    }
   ],
   "source": [
    "# generate a response\n",
    "print(llm.predict(poem_template.format(adjective='sad', subject='ducks')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e79a1d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A bebek in the pond, so still and small\n",
      "Its wings so pure and white, never to be flown at all\n",
      "A life of peace and bliss, never to face any woes\n",
      "A flower waiting to bloom, held back by freezing snow\n",
      "\n",
      "The ripples of its pond, the world without a sound\n",
      "An onlooker stares in awe, but all else goes unseen around\n",
      "A gentle splash, a soft breath, whatever the life ahead\n",
      "A single moment of joy, held in this fragile thread\n",
      "\n",
      "Growing old in the pond, while life around it whizzes\n",
      "A fleeting eternity, never again in its soul to be eased\n",
      "A murky fate always looming, as the years will wear it thin\n",
      "No more ripples, no more dream, a bebek left to give in\n"
     ]
    }
   ],
   "source": [
    "# generate a response\n",
    "print(llm.predict(poem_template.format(adjective='sedih', subject='bebek')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8b00d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a sedih poem about bebek'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_template.format(adjective='sedih', subject='bebek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "19f1c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['makanan', 'bahan'] output_parser=None partial_variables={} template='buatlah resep membuat {makanan} dengan {bahan} and please translate it to english' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"buatlah resep membuat {makanan} dengan {bahan} and please translate it to english\"\n",
    "\n",
    "poem_template = PromptTemplate(\n",
    "    input_variables=[\"makanan\", \"bahan\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "print(poem_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d293469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Bahan-bahan : \n",
      "1 bebek \n",
      "Tepung bumbu \n",
      "Minyak goreng untuk menggoreng\n",
      "\n",
      "Cara Membuat : \n",
      "1. Potong bebek menjadi bagian-bagian yang kecil \n",
      "2. Marinasi potongan bebek dengan tepung bumbu selama 15 - 20 menit \n",
      "3. Panaskan minyak dalam wajan \n",
      "4. Goreng potongan bebek hingga berwarna kuning keemasan. \n",
      "5. Angkat dan sajikan.\n",
      "\n",
      "English Translation : \n",
      "Ingredients : \n",
      "1 duck\n",
      "Seasoning flour\n",
      "Frying oil for frying\n",
      "\n",
      "Instructions : \n",
      "1. Cut the duck into small pieces\n",
      "2. Marinate the duck pieces with seasoning flour for 15 - 20 minutes\n",
      "3. Heat the oil in the pan \n",
      "4. Fry the duck pieces until golden brown. \n",
      "5. Lift and serve.\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(poem_template.format(makanan='gorengan', bahan='bebek')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f47f3e4",
   "metadata": {},
   "source": [
    "We can create a prompt template that acts as a naming consultant for new companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2eb6a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BatikBali.\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template\n",
    "template = \"\"\"\n",
    "I want you to act as a naming consultant for new companies.\n",
    "\n",
    "Here are some examples of good company names:\n",
    "\n",
    "- search engine, Google\n",
    "- social media, Facebook\n",
    "- video sharing, YouTube\n",
    "\n",
    "The name should be short, catchy and easy to remember. \n",
    "\n",
    "What is a good name for a brand that makes {product}?\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object\n",
    "brand_template = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# Format the prompt template with specific industry values\n",
    "batik_prompt = brand_template.format(product='batik')\n",
    "\n",
    "# Print the formatted prompt\n",
    "print(llm.predict(batik_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7b1d9",
   "metadata": {},
   "source": [
    "By using prompt templates, we can easily generate prompts for various industries by filling in the specific values for the variables. This approach allows us to create dynamic and **customizable** prompts for the naming consultant application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8591b4",
   "metadata": {},
   "source": [
    "### Chain\n",
    "\n",
    "Now that we have our model and prompt template, we can combine them by creating a \"chain\". Chains provide a mechanism to link or connect multiple components, such as models, prompts, and other chains.\n",
    "\n",
    "The most common type of chain is an LLMChain, which involves passing the input through a PromptTemplate and then to an LLM. We can create an LLMChain using our existing model and prompt template.\n",
    "\n",
    "For example, if we want to generate a response using our template, our workflow would be as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c6283c",
   "metadata": {},
   "source": [
    "1. Create the prompt based on input with `template_prompt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b46f7e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a brand that makes rendang mozarella?\n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template\n",
    "template_prompt = PromptTemplate\n",
    "    .from_template(\"What is a good name for a brand that makes {product}?\")\n",
    "prompt = template_prompt.format(product=\"rendang mozarella\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a15c7",
   "metadata": {},
   "source": [
    "2. Generate response from prompt with `llm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "929b2771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mozarella Rendang Co.\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e573b33b",
   "metadata": {},
   "source": [
    "We can simplify the workflow by chaining (link) them up with `Chains`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d99df04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LLMChain class from langchain\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5e7ae4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain the prompt template and llm\n",
    "chain = LLMChain(llm=llm, prompt=template_prompt, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "91b0a532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is a good name for a brand that makes rendang mozarella?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Mozarella Rendang Co.\n"
     ]
    }
   ],
   "source": [
    "# Execute the chained model and prompt template\n",
    "print(chain.run('rendang mozarella'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b935d67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "I want you to act as a naming consultant for new companies.\n",
      "\n",
      "Here are some examples of good company names:\n",
      "\n",
      "- search engine, Google\n",
      "- social media, Facebook\n",
      "- video sharing, YouTube\n",
      "\n",
      "The name should be short, catchy and easy to remember. \n",
      "\n",
      "What is a good name for a brand that makes bakso?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "BaksoBros.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "I want you to act as a naming consultant for new companies.\n",
    "\n",
    "Here are some examples of good company names:\n",
    "\n",
    "- search engine, Google\n",
    "- social media, Facebook\n",
    "- video sharing, YouTube\n",
    "\n",
    "The name should be short, catchy and easy to remember. \n",
    "\n",
    "What is a good name for a brand that makes {product}?\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object\n",
    "brand_template = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=brand_template, verbose = True)\n",
    "\n",
    "print(chain.run('bakso'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d6350",
   "metadata": {},
   "source": [
    "The `chain.run()` method generates a response from the LLM model based on the provided input. \n",
    "\n",
    "By chaining the LLM model and the prompt template using the `LLMChain` class, we can conveniently pass inputs through the template and obtain contextually relevant responses from the model. This simple chain allows us to generate responses with just **one line** of code for each new input. Understanding the workings of this basic chain will serve as a solid foundation for working with more intricate chains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34801fcc",
   "metadata": {},
   "source": [
    "### Agents\n",
    "\n",
    "In more complex workflows, it becomes crucial to have the ability to make decisions and choose actions based on the given context. This is where agents come into play.\n",
    "\n",
    "Agents utilize a language model to **determine which actions** to take and in what sequence. They have a set of tools at their disposal, and they continually select, execute, and evaluate these tools until they arrive at the optimal solution. Agents provide a dynamic and adaptable approach to **problem-solving** within the LangChain framework, allowing for more sophisticated and flexible workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9acfa",
   "metadata": {},
   "source": [
    "To load an agent in LangChain, you need to consider the following components:\n",
    "\n",
    "- **LLM/Chat model:** This refers to the language model that powers the agent. It is responsible for generating responses based on the given input. You can choose from various pre-trained models or use your own custom models.\n",
    "\n",
    "- **Tools:** Tools are functions or methods that perform specific tasks within the agent's workflow. These can include actions like Google Search, Database lookup, Python REPL (Read-Eval-Print Loop), or even other chains. LangChain provides a set of predefined tools with their specifications, which you can refer to in the Tools documentation.\n",
    "\n",
    "- **Agent name:** The agent name is a string that identifies a supported agent class. Each agent class is parameterized by the prompt that the language model uses to determine the appropriate action to take. In this context, we will focus on using the standard supported agents, rather than implementing custom agents. You can explore the list of supported agents and their specifications to choose the most suitable one for your application.\n",
    "\n",
    "For the specific example mentioned, we will utilize the `wikipedia` tool to query and retrieve responses based on Wikipedia information. This tool allows the agent to access relevant information from Wikipedia and provide informative responses based on the given input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502430a2",
   "metadata": {},
   "source": [
    "**Import the required modules**: The code starts by importing the necessary modules from LangChain, such as `AgentType`, `initialize_agent`, and `load_tools`. These modules provide the functionalities required to create and configure the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ee5575bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09032c",
   "metadata": {},
   "source": [
    "**Define the language model for the agent**: In this example, the `llm_agent` is initialized with the `OpenAI` class, which represents the language model. The `temperature` parameter determines the level of randomness in the generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6b600323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The language model we're going to use to control the agent.\n",
    "llm_agent = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef985463",
   "metadata": {},
   "source": [
    "**Load the tools**: The `load_tools` function is used to load the desired tools for the agent. In this case, the tools \"wikipedia\" and \"llm-math\" are loaded. \n",
    "\n",
    "> The \"wikipedia\" tool allows the agent to access information from Wikipedia, while the \"llm-math\" tool utilizes the language model for mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "01c66a5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc143d4",
   "metadata": {},
   "source": [
    "**Initialize the agent**: The `initialize_agent` function is called to create an agent instance. It takes the loaded tools, the language model (`llm_agent`), the agent type (`AgentType.ZERO_SHOT_REACT_DESCRIPTION`), and an optional `verbose` parameter. The agent type determines the behavior of the agent, such as generating responses based on descriptions or reacting to user inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a08c0981",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(memory=None, callbacks=None, callback_manager=None, verbose=True, agent=ZeroShotAgent(llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, prompt=PromptTemplate(input_variables=['input', 'agent_scratchpad'], output_parser=None, partial_variables={}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\nWikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Wikipedia, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}', template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all'), output_key='text'), output_parser=MRKLOutputParser(), allowed_tools=['Wikipedia', 'Calculator']), tools=[WikipediaQueryRun(name='Wikipedia', description='A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', args_schema=None, return_direct=False, verbose=False, callbacks=None, callback_manager=None, api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'C:\\\\Users\\\\irvan\\\\anaconda3\\\\envs\\\\lmm_demo\\\\lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=4000)), Tool(name='Calculator', description='Useful for when you need to answer questions about math.', args_schema=None, return_direct=False, verbose=False, callbacks=None, callback_manager=None, func=<bound method Chain.run of LLMMathChain(memory=None, callbacks=None, callback_manager=None, verbose=False, llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: {question}\\n', template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all'), output_key='text'), llm=None, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: {question}\\n', template_format='f-string', validate_template=True), input_key='question', output_key='answer')>, coroutine=<bound method Chain.arun of LLMMathChain(memory=None, callbacks=None, callback_manager=None, verbose=False, llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: {question}\\n', template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all'), output_key='text'), llm=None, prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: {question}\\n', template_format='f-string', validate_template=True), input_key='question', output_key='answer')>)], return_intermediate_steps=False, max_iterations=15, max_execution_time=None, early_stopping_method='force', handle_parsing_errors=False)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm_agent, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3badc",
   "metadata": {},
   "source": [
    "Let's test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5bd60e29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out when Messi joined Barcelona and then calculate his current age raised to the 0.43 power.\n",
      "Action: Wikipedia\n",
      "Action Input: Lionel Messi\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Lionel Messi\n",
      "Summary: Lionel Andrés Messi (Spanish pronunciation: [ljoˈnel anˈdɾes ˈmesi] (listen); born 24 June 1987), also known as Leo Messi, is an Argentine professional footballer who plays as a forward for and captains both Major League Soccer club Inter Miami and the Argentina national team. Widely regarded as one of the greatest players of all time, Messi has won a record seven Ballon d'Or awards and a record six European Golden Shoes, and in 2020 he was named to the Ballon d'Or Dream Team. Until leaving the club in 2021, he had spent his entire professional career with Barcelona, where he won a club-record 34 trophies, including ten La Liga titles, seven Copa del Rey titles and the UEFA Champions League four times. With his country, he won the 2021 Copa América and the 2022 FIFA World Cup. A prolific goalscorer and creative playmaker, Messi holds the records for most goals in La Liga (474), most hat-tricks in La Liga (36) and the UEFA Champions League (eight), and most assists in La Liga (192) and the Copa América (17). He also has the most international goals by a South American male (103). Messi has scored over 800 senior career goals for club and country, and has the most goals by a player for a single club (672).\n",
      "Messi relocated to Spain from Argentina aged 13 to join Barcelona, for whom he made his competitive debut aged 17 in October 2004. He established himself as an integral player for the club within the next three years, and in his first uninterrupted season in 2008–09 he helped Barcelona achieve the first treble in Spanish football; that year, aged 22, Messi won his first Ballon d'Or. Three successful seasons followed, with Messi winning four consecutive Ballons d'Or, making him the first player to win the award four times. During the 2011–12 season, he set the La Liga and European records for most goals scored in a single season, while establishing himself as Barcelona's all-time top scorer. The following two seasons, Messi finished second for the Ballon d'Or behind Cristiano Ronaldo (his perceived career rival), before regaining his best form during the 2014–15 campaign, becoming the all-time top scorer in La Liga and leading Barcelona to a historic second treble, after which he was awarded a fifth Ballon d'Or in 2015. Messi assumed captaincy of Barcelona in 2018, and won a record sixth Ballon d'Or in 2019. Out of contract, he signed for French club Paris Saint-Germain in August 2021, spending two seasons at the club and winning Ligue 1 twice. Messi joined American club Inter Miami in July 2023.\n",
      "An Argentine international, Messi is the country's all-time leading goalscorer and also holds the national record for appearances. At youth level, he won the 2005 FIFA World Youth Championship, finishing the tournament with both the Golden Ball and Golden Shoe, and an Olympic gold medal at the 2008 Summer Olympics. His style of play as a diminutive, left-footed dribbler drew comparisons with his compatriot Diego Maradona, who described Messi as his successor. After his senior debut in August 2005, Messi became the youngest Argentine to play and score in a FIFA World Cup (2006), and reached the final of the 2007 Copa América, where he was named young player of the tournament. As the squad's captain from August 2011, he led Argentina to three consecutive finals: the 2014 FIFA World Cup, for which he won the Golden Ball, the 2015 Copa América, winning the Golden Ball, and the 2016 Copa América. After announcing his international retirement in 2016, he reversed his decision and led his country to qualification for the 2018 FIFA World Cup, a third-place finish at the 2019 Copa América, and victory in the 2021 Copa América, while winning the Golden Ball and Golden Boot for the latter. For this achievement, Messi received a record-extending seventh Ballon d'Or in 2021. In 2022, he led Argentina to win the 2022 FIFA World Cup, where he won a record second Golden Ball, scored seven goals including two in the final, a\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know that Lionel Messi joined Barcelona in 2004 and is currently 34 years old.\n",
      "Action: Calculator\n",
      "Action Input: 34^0.43\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 4.555498776452875\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Lionel Messi joined Barcelona in 2004 and is currently 34 years old, with his age raised to the 0.43 power being 4.555498776452875.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lionel Messi joined Barcelona in 2004 and is currently 34 years old, with his age raised to the 0.43 power being 4.555498776452875.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What year did Lionel Messi Joined Barcelona? What is his current age raised to the 0.43 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0ddfe9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who the president of Indonesia is\n",
      "Action: Wikipedia\n",
      "Action Input: President of Indonesia\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: President of Indonesia\n",
      "Summary: The president of the Republic of Indonesia (Indonesian: Presiden Republik Indonesia) is both the head of state and the head of government of the Republic of Indonesia. The president leads the executive branch of the Indonesian government and is the commander-in-chief of the Indonesian National Armed Forces. Since 2004, the president and vice president are directly elected to a five-year term, once renewable, allowing for a maximum of 10 years in office.\n",
      "Joko Widodo is the seventh and current president of Indonesia. He assumed office on 20 October 2014.\n",
      "\n",
      "Page: List of presidents of Indonesia\n",
      "Summary: The president is the head of state and also head of government of the Republic of Indonesia. The president leads the executive branch of the Indonesian government and is the commander-in-chief of the Indonesian National Armed Forces. Since 2004, the president and vice president are directly elected to a five-year term.\n",
      "The presidency was established during the formulation of the 1945 constitution by the Investigating Committee for Preparatory Work for Independence (BPUPK), a body established by the occupying Japanese 16th Army on 1 March 1945 to work on \"preparations for independence in the region of the government of this island of Java\". On 18 August 1945, the Preparatory Committee for Indonesian Independence (PPKI), which was created on 7 August to replace the BPUPK, selected Sukarno as the country's first president.\n",
      "\n",
      "Page: Vice President of Indonesia\n",
      "Summary: The vice president of the Republic of Indonesia (Indonesian: Wakil Presiden Republik Indonesia) is second-highest officer in the executive branch of the Indonesian government, after the president, and ranks first in the presidential line of succession. Since 2004, the president and vice president are directly elected to a five-year term.\n",
      "Ma'ruf Amin is the 13th and current vice president of Indonesia. He assumed office on 20 October 2019.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Joko Widodo is the seventh and current president of Indonesia, and Ma'ruf Amin is the 13th and current vice president of Indonesia.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Joko Widodo is the seventh and current president of Indonesia, and Ma'ruf Amin is the 13th and current vice president of Indonesia.\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run('Siapa president Republik Indonesia')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c2bee",
   "metadata": {},
   "source": [
    "By executing these steps, we establish an agent that can utilize various tools and interact with the chosen language model to generate contextually relevant responses based on the given input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98be5c8",
   "metadata": {},
   "source": [
    "# Build Question Answering System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8747e7e",
   "metadata": {},
   "source": [
    "## Introduction to Question-Answer System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880d77d",
   "metadata": {},
   "source": [
    "As we know, LangChain is an open-source library that provides developers with powerful tools for building applications using Large Language Models (LLMs). In our previous example, we saw how we could use an LLM to generate responses based on a given question. However, there may be cases where we need to ask more specific questions related to our business domain. For instance, we might want to ask the LLM about our company's top revenue-generating product.\n",
    "\n",
    "LLMs have certain limitations when it comes to specific contextual knowledge, as they are trained on a vast amount of general information. To overcome this limitation, we can provide additional documents or context to the LLM. The idea is to retrieve relevant documents related to our question from a corpus or database and then pass them along with the original question to the LLM. This allows the LLM to generate a response that is informed by the specific information contained in the retrieved documents.\n",
    "\n",
    "These documents can come from various sources such as databases, PDF files, plain text files, or even information extracted from websites. By connecting and feeding these documents to the LLM, we can build a powerful Question-Answer System that leverages the LLM's language generation capabilities while incorporating domain-specific knowledge.\n",
    "\n",
    "In this section, we will explore how to connect and feed a database and text information to LLM to build Question-Answer System that can provide contextually relevant answers to specific business-related questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75bb9f",
   "metadata": {},
   "source": [
    "## Database\n",
    "\n",
    "### Connecting Database\n",
    "\n",
    "LangChain provide function that connect database to LLM, it called SQL Database. It also provide a function to chaining between the database, model llm and an agent that will execute SQL query based on natural language prompt\n",
    "\n",
    "The integration process involves establishing a **connection** to the database, **defining** the necessary SQL queries, and **utilizing** the LLM and agent to execute those queries based on user prompts. This allows for a user-friendly and intuitive way to interact with databases, leveraging the language model's capabilities to understand and process natural language input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07178b6",
   "metadata": {},
   "source": [
    "To use LangChain to connect with a database, we need to utilize the `SQLDatabase` class. This class allows us to establish a connection between LangChain and an SQL database, enabling seamless integration of natural language queries and execution of SQL commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1568ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SQLDatabase, SQLDatabaseChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d75f2",
   "metadata": {},
   "source": [
    "At this part we need to load the data, we will use the chinook data from our academy class as example. You need to explicitly explain what kind of database you load, for example `sqlite:///`.\n",
    "\n",
    "Then we can just load the database using `SQLDatabase` from `langchain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0b17e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dburi = \"sqlite:///data_input/chinook.db\"\n",
    "\n",
    "db = SQLDatabase.from_uri(dburi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d075a8",
   "metadata": {},
   "source": [
    "> `from_uri()` allows us to interact with the database and perform various operations such as querying data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c211b",
   "metadata": {},
   "source": [
    "After that, we chain the `db` to the model, creating an agent that can search for answers in the Chinook database based on the prompt input.\n",
    "\n",
    "Let's try a prompt to find out how many rows are there in the \"tracks\" table of this database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8aa2d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0) # parameter temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ada134e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "How many rows is in the tracks table of this db?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT COUNT(*) FROM tracks;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(3503,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThe tracks table has 3503 rows.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The tracks table has 3503 rows.'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)\n",
    "\n",
    "db_chain.run(\"How many rows is in the tracks table of this db?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "05430ef5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "please describe this database for me?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT \"ArtistId\", \"Name\" FROM artists\n",
      "          UNION\n",
      "          SELECT \"EmployeeId\", \"LastName\" FROM employees\n",
      "          UNION\n",
      "          SELECT \"GenreId\", \"Name\" FROM genres\n",
      "          UNION\n",
      "          SELECT \"MediaTypeId\", \"Name\" FROM media_types\n",
      "          UNION\n",
      "          SELECT \"PlaylistId\", \"Name\" FROM playlists\n",
      "          UNION\n",
      "          SELECT \"AlbumId\", \"Title\" FROM albums\n",
      "          UNION\n",
      "          SELECT \"CustomerId\", \"LastName\" FROM customers\n",
      "          UNION\n",
      "          SELECT \"InvoiceId\", \"InvoiceDate\" FROM invoices\n",
      "          UNION\n",
      "          SELECT \"TrackId\", \"Name\" FROM tracks\n",
      "          UNION\n",
      "          SELECT \"InvoiceLineId\", \"UnitPrice\" FROM invoice_items\n",
      "          UNION\n",
      "          SELECT \"PlaylistId\", \"TrackId\" FROM playlist_track\n",
      "          LIMIT 5;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(1, 0.99), (1, 1), (1, 2), (1, 3), (1, 4)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThis database contains information about artists, employees, genres, media types, playlists, albums, customers, invoices, tracks, invoice items, and playlist tracks.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This database contains information about artists, employees, genres, media types, playlists, albums, customers, invoices, tracks, invoice items, and playlist tracks.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain.run(\"please describe this database for me? please avoid ambigous name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "942ee6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "can you please make a dataset that contains track info, artist name, and genres\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT t.\"Name\" AS TrackName, a.\"Name\" AS ArtistName, g.\"Name\" AS GenreName\n",
      "FROM tracks t\n",
      "INNER JOIN albums al ON t.\"AlbumId\" = al.\"AlbumId\"\n",
      "INNER JOIN artists a ON al.\"ArtistId\" = a.\"ArtistId\"\n",
      "INNER JOIN genres g ON t.\"GenreId\" = g.\"GenreId\"\n",
      "LIMIT 5;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[('For Those About To Rock (We Salute You)', 'AC/DC', 'Rock'), ('Put The Finger On You', 'AC/DC', 'Rock'), (\"Let's Get It Up\", 'AC/DC', 'Rock'), ('Inject The Venom', 'AC/DC', 'Rock'), ('Snowballed', 'AC/DC', 'Rock')]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThe dataset contains track info, artist name, and genres.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The dataset contains track info, artist name, and genres.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain.run(\"can you please make a dataset that contains track info, artist name, and genres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b05bf",
   "metadata": {},
   "source": [
    "Notice that the output contains several components: \n",
    "- `SQLQuery`, which provides information about the process the model used to search for the answer using SQL.\n",
    "- `SQLResult`, which represents the result obtained from executing the SQL query on our database.\n",
    "- `Answer`, which converts the `SQLResult` into natural language and displays it as the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9505375b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7663bfcd",
   "metadata": {},
   "source": [
    "### Basics of Building Question-Answer System using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0283f71d",
   "metadata": {},
   "source": [
    "The `SqlDatabaseChain` is a powerful tool that enables us to answer questions by querying a SQL database. It seamlessly integrates the language model's capabilities with SQL queries, providing a convenient way to retrieve specific information from structured data stored in a database. With the `SqlDatabaseChain`, we can easily harness the power of both the language model and the SQL database to build a robust question-answering system for your data-driven applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2685023d",
   "metadata": {},
   "source": [
    "Another example we use the question:\n",
    "\n",
    "> all sales in rock genre in 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "296b75a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "all sales in rock genre in 2012 that happened in germany based on invoice please dont use limit statement\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT i.InvoiceId, i.InvoiceDate, i.BillingCountry, t.GenreId, t.Name \n",
      "FROM invoices i \n",
      "INNER JOIN invoice_items ii ON i.InvoiceId = ii.InvoiceId \n",
      "INNER JOIN tracks t ON ii.TrackId = t.TrackId \n",
      "WHERE t.GenreId = 1 \n",
      "AND strftime('%Y', i.InvoiceDate) = '2012' \n",
      "AND i.BillingCountry = 'Germany'\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(291, '2012-06-30 00:00:00', 'Germany', 1, 'Creep'), (291, '2012-06-30 00:00:00', 'Germany', 1, 'Dark Corners'), (293, '2012-07-13 00:00:00', 'Germany', 1, 'Boris The Spider')]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThere were 3 sales in rock genre in 2012 that happened in Germany based on invoice.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There were 3 sales in rock genre in 2012 that happened in Germany based on invoice.'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain.run(\"all sales in rock genre in 2012 that happened in germany based on invoice please dont use limit statement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efface3e",
   "metadata": {},
   "source": [
    "Kalau mau jadi data frame bisa copy dari sana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f147ed8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceId</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>BillingCountry</th>\n",
       "      <th>GenreId</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>291</td>\n",
       "      <td>2012-06-30 00:00:00</td>\n",
       "      <td>Germany</td>\n",
       "      <td>1</td>\n",
       "      <td>Creep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>291</td>\n",
       "      <td>2012-06-30 00:00:00</td>\n",
       "      <td>Germany</td>\n",
       "      <td>1</td>\n",
       "      <td>Dark Corners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293</td>\n",
       "      <td>2012-07-13 00:00:00</td>\n",
       "      <td>Germany</td>\n",
       "      <td>1</td>\n",
       "      <td>Boris The Spider</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   InvoiceId          InvoiceDate BillingCountry  GenreId              Name\n",
       "0        291  2012-06-30 00:00:00        Germany        1             Creep\n",
       "1        291  2012-06-30 00:00:00        Germany        1      Dark Corners\n",
       "2        293  2012-07-13 00:00:00        Germany        1  Boris The Spider"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('data_input/chinook.db')\n",
    "\n",
    "pd.read_sql_query('''SELECT i.InvoiceId, i.InvoiceDate, i.BillingCountry, t.GenreId, t.Name \n",
    "FROM invoices i \n",
    "INNER JOIN invoice_items ii ON i.InvoiceId = ii.InvoiceId \n",
    "INNER JOIN tracks t ON ii.TrackId = t.TrackId \n",
    "WHERE t.GenreId = 1 \n",
    "AND strftime('%Y', i.InvoiceDate) = '2012' \n",
    "AND i.BillingCountry = 'Germany' \n",
    "''', con = conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c3a71b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "We want the returned DataFrame to contain only the Pop genre and only when the UnitPrice of the track is 0.99\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT * FROM tracks t JOIN genres g ON t.GenreId = g.GenreId WHERE g.Name = 'Pop' AND t.UnitPrice = 0.99 LIMIT 5;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(323, 'Dig-Dig, Lambe-Lambe (Ao Vivo)', 29, 1, 9, 'Cassiano Costa/Cintia Maviane/J.F./Lucas Costa', 205479, 6892516, 0.99, 9, 'Pop'), (324, 'Pererê', 29, 1, 9, 'Augusto Conceição/Chiclete Com Banana', 198661, 6643207, 0.99, 9, 'Pop'), (325, 'TriboTchan', 29, 1, 9, 'Cal Adan/Paulo Levi', 194194, 6507950, 0.99, 9, 'Pop'), (326, 'Tapa Aqui, Descobre Ali', 29, 1, 9, 'Paulo Levi/W. Rangel', 188630, 6327391, 0.99, 9, 'Pop'), (327, 'Daniela', 29, 1, 9, 'Jorge Cardoso/Pierre Onasis', 230791, 7748006, 0.99, 9, 'Pop')]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThe DataFrame contains 5 tracks from the Pop genre with a UnitPrice of 0.99.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The DataFrame contains 5 tracks from the Pop genre with a UnitPrice of 0.99.'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain.run(\"We want the returned DataFrame to contain only the Pop genre and only when the UnitPrice of the track is 0.99\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6e7d872d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "Tampilkan lagu dengan Genre Pop\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT \"Name\" FROM tracks WHERE \"GenreId\" = (SELECT \"GenreId\" FROM genres WHERE \"Name\" = 'Pop') LIMIT 5;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[('Dig-Dig, Lambe-Lambe (Ao Vivo)',), ('Pererê',), ('TriboTchan',), ('Tapa Aqui, Descobre Ali',), ('Daniela',)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mLagu dengan Genre Pop adalah Dig-Dig, Lambe-Lambe (Ao Vivo), Pererê, TriboTchan, Tapa Aqui, Descobre Ali, dan Daniela.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lagu dengan Genre Pop adalah Dig-Dig, Lambe-Lambe (Ao Vivo), Pererê, TriboTchan, Tapa Aqui, Descobre Ali, dan Daniela.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain.run(\"Tampilkan lagu dengan Genre Pop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a51f7c",
   "metadata": {},
   "source": [
    "## Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd12ebb5",
   "metadata": {},
   "source": [
    "### Connecting to CSV\n",
    "\n",
    "Structured data is not only stored in database files; it can also be stored in other formats such as `.xlsx` and `.csv`, which represent data in a tabular form with columns and rows. In addition to providing agents to generate answers from databases using SQL based on natural language prompts, LangChain also offers agents to generate answers based on **tabular structured data sources**, such as CSV files. In this section, we will demonstrate how to utilize the agent for CSV data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7400c8",
   "metadata": {},
   "source": [
    "To begin, let's define the file path of our dataset `rice.csv`, which contains rice transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c4884420",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data_input/rice.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73087995",
   "metadata": {},
   "source": [
    "Next, we will create an agent specifically designed for working with CSV data. This agent will allow us **to query and retrieve information from the `rice.csv` dataset**. Since we are using the same LLM model as in the SQL part, there is no need to redefine the LLM. We can utilize the existing LLM model for our CSV agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d8432e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_csv_agent\n",
    "agent = create_csv_agent(llm, filepath, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576935e7",
   "metadata": {},
   "source": [
    "Then we just run ask the question about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "67cb93e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: saya harus menghitung jumlah transaksi yang terjadi di setiap format\n",
      "Action: python_repl_ast\n",
      "Action Input: df.groupby('format')['receipt_id'].count()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mformat\n",
      "hypermarket     999\n",
      "minimarket     7088\n",
      "supermarket    3913\n",
      "Name: receipt_id, dtype: int64\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Saya sekarang tahu jawaban akhir\n",
      "Final Answer: Hypermarket memiliki 999 transaksi, Minimarket memiliki 7088 transaksi, dan Supermarket memiliki 3913 transaksi.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hypermarket memiliki 999 transaksi, Minimarket memiliki 7088 transaksi, dan Supermarket memiliki 3913 transaksi.'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"berikan detail banyaknya transaksi yang terjadi di setiap format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "030d9dcd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Saya harus mengecek apakah ada nilai yang masih NA\n",
      "Action: python_repl_ast\n",
      "Action Input: df.isna().sum()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mUnnamed: 0          0\n",
      "receipt_id          0\n",
      "receipts_item_id    0\n",
      "purchase_time       0\n",
      "category            0\n",
      "sub_category        0\n",
      "format              0\n",
      "unit_price          0\n",
      "discount            0\n",
      "quantity            0\n",
      "yearmonth           0\n",
      "dtype: int64\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Saya sekarang tahu jawabannya\n",
      "Final Answer: Tidak, tidak ada nilai yang masih NA.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tidak, tidak ada nilai yang masih NA.'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"apakah di data ini ada yang masih NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "543a2d16",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Saya harus mencari tahu apakah ada data yang sama\n",
      "Action: python_repl_ast\n",
      "Action Input: df.duplicated()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m0        False\n",
      "1        False\n",
      "2        False\n",
      "3        False\n",
      "4        False\n",
      "         ...  \n",
      "11995    False\n",
      "11996    False\n",
      "11997    False\n",
      "11998    False\n",
      "11999    False\n",
      "Length: 12000, dtype: bool\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Saya sekarang tahu jawabannya\n",
      "Final Answer: Tidak ada data yang duplicate.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tidak ada data yang duplicate.'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run('apakah ada data yang duplicate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5fac5033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: saya harus menemukan data yang duplikat\n",
      "Action: python_repl_ast\n",
      "Action Input: df.drop_duplicates()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m       Unnamed: 0  receipt_id  receipts_item_id     purchase_time category   \n",
      "0               1     9622257          32369294   7/22/2018 21:19     Rice  \\\n",
      "1               2     9446359          31885876   7/15/2018 16:17     Rice   \n",
      "2               3     9470290          31930241   7/15/2018 12:12     Rice   \n",
      "3               4     9643416          32418582    7/24/2018 8:27     Rice   \n",
      "4               5     9692093          32561236   7/26/2018 11:28     Rice   \n",
      "...           ...         ...               ...               ...      ...   \n",
      "11995       11996     5760491          17555486  12/15/2017 21:06     Rice   \n",
      "11996       11997     5598782          16999147   12/2/2017 14:12     Rice   \n",
      "11997       11998     5735850          17434503  12/13/2017 19:17     Rice   \n",
      "11998       11999     5678748          17317935   12/8/2017 22:04     Rice   \n",
      "11999       12000     5702411          17341559  12/10/2017 17:04     Rice   \n",
      "\n",
      "      sub_category       format  unit_price  discount  quantity yearmonth  \n",
      "0             Rice  supermarket    128000.0         0         1   2018-07  \n",
      "1             Rice   minimarket    102750.0         0         1   2018-07  \n",
      "2             Rice  supermarket     64000.0         0         3   2018-07  \n",
      "3             Rice   minimarket     65000.0         0         1   2018-07  \n",
      "4             Rice  supermarket    124500.0         0         1   2018-07  \n",
      "...            ...          ...         ...       ...       ...       ...  \n",
      "11995         Rice  supermarket    128000.0         0         1   2017-12  \n",
      "11996         Rice   minimarket     64000.0         0         1   2017-12  \n",
      "11997         Rice  supermarket     59990.0      3000         1   2017-12  \n",
      "11998         Rice   minimarket     59500.0      3000         1   2017-12  \n",
      "11999         Rice   minimarket     59500.0      3000         1   2017-12  \n",
      "\n",
      "[12000 rows x 11 columns]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Saya sekarang tahu jawaban akhir\n",
      "Final Answer: Data telah dibersihkan dari duplikat.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data telah dibersihkan dari duplikat.'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run('coba bersihkan data yang duplikat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3a5f0efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: saya harus mencari rata-rata harga beras di supermarket dan minimarket\n",
      "Action: python_repl_ast\n",
      "Action Input: df.groupby('format')['unit_price'].mean()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mformat\n",
      "hypermarket    71205.458458\n",
      "minimarket     67135.569554\n",
      "supermarket    74921.182150\n",
      "Name: unit_price, dtype: float64\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m rekomendasi saya adalah minimarket\n",
      "Final Answer: Rekomendasi saya adalah untuk membeli beras di minimarket.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Rekomendasi saya adalah untuk membeli beras di minimarket.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run('berikan saya rekomendasi dimana tempat paling murah untuk saya membeli beras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "efd8d515",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Saya harus mencari tahu informasi tentang data ini\n",
      "Action: python_repl_ast\n",
      "Action Input: df.info()\u001b[0m<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12000 entries, 0 to 11999\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        12000 non-null  int64  \n",
      " 1   receipt_id        12000 non-null  int64  \n",
      " 2   receipts_item_id  12000 non-null  int64  \n",
      " 3   purchase_time     12000 non-null  object \n",
      " 4   category          12000 non-null  object \n",
      " 5   sub_category      12000 non-null  object \n",
      " 6   format            12000 non-null  object \n",
      " 7   unit_price        12000 non-null  float64\n",
      " 8   discount          12000 non-null  int64  \n",
      " 9   quantity          12000 non-null  int64  \n",
      " 10  yearmonth         12000 non-null  object \n",
      "dtypes: float64(1), int64(5), object(5)\n",
      "memory usage: 1.0+ MB\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mNone\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Saya harus mencari tahu informasi tentang kolom-kolom dalam data ini\n",
      "Action: python_repl_ast\n",
      "Action Input: df.columns\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIndex(['Unnamed: 0', 'receipt_id', 'receipts_item_id', 'purchase_time',\n",
      "       'category', 'sub_category', 'format', 'unit_price', 'discount',\n",
      "       'quantity', 'yearmonth'],\n",
      "      dtype='object')\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Saya sekarang tahu informasi tentang data ini\n",
      "Final Answer: Data ini berisi informasi tentang pembelian, termasuk nomor resi, ID item, waktu pembelian, kategori, sub-kategori, format, harga unit, diskon, jumlah, dan bulan tahun pembelian.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data ini berisi informasi tentang pembelian, termasuk nomor resi, ID item, waktu pembelian, kategori, sub-kategori, format, harga unit, diskon, jumlah, dan bulan tahun pembelian.'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run('coba deskripsikan isi dari data ini apa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c651d2fc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: saya harus menggunakan fungsi statistik untuk menghasilkan ringkasan\n",
      "Action: python_repl_ast\n",
      "Action Input: df.describe()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m        Unnamed: 0    receipt_id  receipts_item_id     unit_price   \n",
      "count  12000.00000  1.200000e+04      1.200000e+04   12000.000000  \\\n",
      "mean    6000.50000  7.650135e+06      2.457950e+07   70013.146313   \n",
      "std     3464.24595  1.873838e+06      7.171105e+06   29905.391437   \n",
      "min        1.00000  3.173994e+06      9.282023e+06    9395.000000   \n",
      "25%     3000.75000  5.983209e+06      1.820694e+07   61900.000000   \n",
      "50%     6000.50000  7.443618e+06      2.234337e+07   63500.000000   \n",
      "75%     9000.25000  9.149786e+06      3.108379e+07   66000.000000   \n",
      "max    12000.00000  1.146321e+07      3.842939e+07  219400.000000   \n",
      "\n",
      "            discount      quantity  \n",
      "count   12000.000000  12000.000000  \n",
      "mean      835.305750      1.332917  \n",
      "std      6207.475704      0.980304  \n",
      "min     -4100.000000      1.000000  \n",
      "25%         0.000000      1.000000  \n",
      "50%         0.000000      1.000000  \n",
      "75%         0.000000      1.000000  \n",
      "max    320000.000000     19.000000  \u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Saya sekarang tahu jawaban akhir\n",
      "Final Answer: Ringkasan statistik dari dataframe df adalah sebagai berikut: jumlah, rata-rata, standar deviasi, nilai minimum, nilai tengah, nilai maksimum, diskon, dan kuantitas.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ringkasan statistik dari dataframe df adalah sebagai berikut: jumlah, rata-rata, standar deviasi, nilai minimum, nilai tengah, nilai maksimum, diskon, dan kuantitas.'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run('berikan saya ringkasan statistika dari data ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a14a5",
   "metadata": {},
   "source": [
    "Notice that there are additional components in the output:\n",
    "- `Thought`: This represents the **agent's thought** process on how to solve the problem based on the given prompt. It provides insights into the agent's decision-making and the reasoning behind its actions.\n",
    "- `Action`: This describes the **actions taken** by the agent to solve the problem. In this case, it involves using the `python_repl_ast` tool, which is a Python shell. It also indicates the specific `pandas` command used by the agent to extract the result from the CSV data.\n",
    "- `Final Answer`: This is the natural language representation of the **answer** derived from the result of the `Action Input`. It presents the final response to the prompt in a human-readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f302e4",
   "metadata": {},
   "source": [
    "## Unstructured Data\n",
    "\n",
    "The company stores not only structured data but also unstructured data, such as **meeting summaries, task reports, and product descriptions**. When we need to retrieve information or ask questions about these documents, it usually requires manual searching and reading through them.\n",
    "\n",
    "However, what if we could leverage the power of LLM models to find the answers for us? What if we have unique documents or regulations specific to our company? With OpenAI Embeddings, we can add our own company-specific information to the LLM model. This allows us to build a question-answering system that can provide answers based on our own company documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c96738ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader #untuk load data\n",
    "from langchain.text_splitter import CharacterTextSplitter # untuk split kata kata\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ea5b0",
   "metadata": {},
   "source": [
    "Implementing LangChain involves utilizing various modules and components to work with documents and text data effectively, we can use:\n",
    "\n",
    "- `DirectoryLoader` and `TextLoader` are document loaders that allow us to load documents from a directory or individual text files.\n",
    "- `CharacterTextSplitter` is a text splitter that helps us split text into smaller units, such as characters.\n",
    "- `OpenAIEmbeddings` is a module that provides embeddings for our text data. Embeddings are numerical representations of words or sentences that capture their meaning.\n",
    "- `Chroma` is a vector store that stores the embeddings in a way that allows for efficient similarity searches.\n",
    "- `RetrievalQA` is a question-answering module that uses the embeddings and vector store to retrieve answers from the documents.\n",
    "\n",
    "By creating an instance of `OpenAIEmbeddings()` and assigning it to the variable `embeddings`, we can now use it to encode and represent our text data. This enables us to build powerful question-answering systems using the RetrievalQA module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8517747",
   "metadata": {},
   "source": [
    "Let's utilize the `summary.txt` file, which contains summaries of coal news for Australia, Indonesia, and China. We can use the `TextLoader` module to load this text file and process its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "532c8497",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('data_input/summary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c3a1a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=2500, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "#texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2ebe3",
   "metadata": {},
   "source": [
    "The `texts` object contains the result of splitting the loaded documents into smaller chunks of text. Each chunk of text is limited to a maximum of 2500 characters (`chunk_size`) and there is no overlap between the chunks (`chunk_overlap`). \n",
    "\n",
    "The purpose of splitting the text into smaller chunks is to facilitate processing and analysis of the document content in a more manageable way. The `texts` object now holds these smaller text chunks, which can be further utilized for various text processing tasks such as information retrieval or natural language understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "19624cb8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Summary for Australia:\\n\\n- Australia\\'s coal and gas exports may reduce by half within the next five years due to the passing of its peak and the efforts of Asian countries to decrease greenhouse gas emissions. The earnings of minerals and energy exports are predicted to reach $464bn in 2022-23 from $128bn in thermal coal exports and $91bn in liquidified natural gas (LNG) exports. These figures have resulted from the global energy crisis caused by Russia\\'s invasion of Ukraine, leading to high fossil fuel prices, causing the replacement of Russian gas with alternative supplies in northern hemisphere nations. \\n\\n- The seaborne coal market grew by 5.9% year-on-year to 1208 million tonnes in 2022, reversing the negative trend of previous years, according to shipbroker Banchero Costa. Although Australia\\'s coal exports declined by 5% in 2022 due to China\\'s adoption  of alternative markets, relations between the two countries have mended and coal shipments are expected to resume. Indonesia is now the largest exporter of coal globally, with a 32.2% share in 2022 compared to Australia\\'s 28.2%. In terms of export destinations, Japan was the top with 115 million tonnes in 2021, while exports to India rose by 13.6% but declined by 3.3% to China.\\n \\n- Coal producers are in talks with the government of New South Wales, following the government\\'s announcement that coal miners should reserve up to 10% of production for domestic supply to control rising energy costs in Australia. Coal producers are expected to be minimally impacted since spot supplies are limited, limiting requisition under the rule. The state is expanding an existing system that requires some coal-mining companies to reserve supply. Exports of coal are essential to the Australian economy, with 80% of the country\\'s coal exported, yet the move comes as coal prices rise nearly 50% YoY.\\n \\n- After a year of frozen relations between China and Australia, there is hope for a thaw in their relationship. Chinaâ€™s Foreign Minister, Wang Yi, met with a delegation from Australia to mark the 50th anniversary of bilateral relations and stated that the countries have no \"fundamental conflicts of interest\". Australia is hoping to resume trade in commodities such as iron ore and coking coal. While it is still uncertain whether a trade recovery will happen, analysts and experts from both countries have expressed optimism. Trade data shows that in 2021 China has already imported 694 million tons of iron ore from Australia, accounting for 62% of total imports.\\n\\nSummary for Indonesia\\n\\n- Indonesia plans to produce a record 695 million tonnes of coal in 2023 and export 518 million tonnes, according to Energy and Mineral Resources Minister Arifin Tasrif. The country\\'s coal-fired energy supply makes up more than half of its total, though the government aims for a net-zero emissions goal by 2060. Domestic coal consumption in Indonesia is projected at 177 million tonnes this year, down from 193 million tonnes in 2022. \\n\\n- Global seaborne coal loadings increased by 5.9% year on year to 1204.9 million tonnes in 2022, with Indonesia\\'s exports rising by 21.2% to 388.9 million tonnes, according to shipbroker Banchero Costa. The EU\\'s coal imports jumped 33.9% year on year to 116.5 million tonnes, while India took 13.6% more coal from Indonesia, receiving a total of 203.8 million tonnes. Coal shipments to China fell by 3.2% to 234.7 million tonnes, and Australian exports declined by 5.0%. \\n\\n- Ford is investing in a $4.5bn nickel processing facility in Indonesia, in partnership with PT Vale Indonesia and China\\'s Zhejiang Huayou Cobalt, in order to secure a supply of nickel needed for EV batteries. The facility is set to begin commercial operations in 2026 and is expected to help Ford achieve its target of producing around two million electric vehicles in that year. China\\'s services sector reached its highest level in over a decade in March, providing a promising signal for the global economy, which is relying on Chinese consumers to drive growth. Coal prices have continued to fall, with Central Appalachian coal down 57% from the start of the year. \\n\\n- Bank Indonesia predicts that Indonesia\\'s economy will continue to experience strong growth, with a range of 4.5-5.3% expected in 2023, despite global economic uncertainty. The country\\'s economy grew by 5.31% in 2022, driven by a surplus in the trade balance, controlled inflation, increased credit growth, and healthy financial systems. Credit growth is expected to increase by 10-12% in 2023, and increased domestic and export demand are projected to strengthen household consumption and drive economic growth. Non-oil and gas exports, including coal, metal ore, and crude palm oil, have grown rapidly. \\n\\n- Indonesia aims to produce 694 million tons of coal in 2021 to fulfill both domestic and export demands, said the country\\'s Ministry of Energy and Mineral Resources. Last year, the nation produced 627 million tons, with 4-5 million tons exported to Europe, compared with 500,000 tons in previous years, the Indonesian Coal Mining Association said. European coal demand is expected to stay strong next year.\\n\\n- Indonesia\\'s new ban on coal exports, implemented to ensure adequate supply for its state-owned electricity companies, is expected to disrupt Supramax and Panamax markets in the Pacific region. The country exports around 400 million metric tonnes of thermal coal each year to countries including China, India, Japan, South Korea and Vietnam. The ban, which has seen bulkers unable to sail out of Indonesian ports, is likely to result in a tonnage surfeit in the Asia-Pacific, leading to lower shipping rates, particularly in East Coast South America (ECSA) and the Indian Ocean. \\n\\nSummary for China:\\n\\n- China\\'s coal industry is expected to see stronger trade as the country ramps up energy supplies and stabilizes prices, with coal accounting for 56% of the country\\'s total primary energy consumption. In July 2022, China\\'s coal and lignite exports rose by 171.6% YoY to 230,000 tons. Indonesia is the largest exporter of coal to China, accounting for 58.3% of total imports, followed by Russia at 23.3% and Mongolia at 10%. Australian coal miners are expected to see a pick up in exports to China, while technology and investment from China is set to play a vital role in Indonesian coal deep-processing. \\n\\n- China\\'s imports of Russian coal rose to 8.54 million tonnes in August due to soaring energy demand caused by extremely hot weather. This trade marked the highest volume since data collection began in 2017, and was up 57% compared with the same period last year. Chinaâ€™s purchase of Russian imports rose by 60% to $11.2bn in August on the back of surging demand for oil, coal and gas. Meanwhile, bilateral trade between China and Russia reached $117.2bn in the first eight months of 2022; up over 31% YoY. \\n\\n- More than 50% of China\\'s coking coal imports this year are expected to come from Mongolia, according to coking coal analyst Li Xiaoyun from MySteel consultancy. Mongolia is projected to export approximately 40m to 50m tonnes of coking coal to China in 2022, making it the largest coking coal seller to China for the third consecutive year after it replaced Australia in 2021, according to MySteel\\'s data. However, China\\'s total shipments for coking coal imports are expected to have a minor increase due to imports growth from Australia and Indonesia, Li added. \\n\\n- China added 38.4 GW of new coal-fired power capacity in 2020, more than the rest of the world combined, according to research by US think tank Global Energy Monitor and the Centre for Research on Energy and Clean Air in Helsinki. The capacity increase means China has not been cutting emissions despite last year\\'s pledge by President Xi Jinping that the country would be carbon neutral by 2060. The government has come under criticism from environmentalists for allowing coal-fired plants to be built in polluted regions and developing projects in greener areas more slowly. \\n\\n- China increased its coal-fired power capacity by 42.9 GW, or 4.5%, in the 18 months to June 2019, according to a report by Global Energy Monitor. The study also found that another 121.3 GW of coal-fired power plants are under construction in China, which has pledged to reduce its coal usage. However, the countryâ€™s absolute coal consumption has still increased in line with rising energy demand. China accounts for more than 40% of the world\\'s total coal generation capacity.', metadata={'source': 'data_input/summary.txt'})]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb1f910",
   "metadata": {},
   "source": [
    "Next, we create a `docsearch` object using the `Chroma.from_documents` method to build a vector store from the `texts`. This `docsearch` object enables efficient similarity search and retrieval of documents based on their text representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0af267fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    }
   ],
   "source": [
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a32ba",
   "metadata": {},
   "source": [
    "To create a question-answering system, we use the `RetrievalQA.from_chain_type` method to construct a `qa_chain` object. This chain type leverages an LLM model, specified by `OpenAI()`, and utilizes the `docsearch` object as a retriever to find relevant documents for answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "05c33aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(), \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac28c83",
   "metadata": {},
   "source": [
    "Now we can use the `qa_chain.run()` method to interact with the question-answering system and obtain answers based on the given prompts or questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5e81c99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Australia's coal and gas exports may reduce by half within the next five years due to the efforts of Asian countries to decrease greenhouse gas emissions. Coal producers are in talks with the government of New South Wales, following the government's announcement that coal miners should reserve up to 10% of production for domestic supply to control rising energy costs in Australia.\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.run(\"What are the effects of legislations surrounding emissions on the Australia coal market?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "817f301f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes, Indonesia has implemented a ban on coal exports to ensure adequate supply for its state-owned electricity companies.'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.run(\"Is there an export ban on Coal in Indonesia? Why?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4a07af59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The main exporters of coal to China are Indonesia, Russia and Mongolia. Indonesia is the largest exporter, accounting for 58.3% of total imports. Technology and investment from China is set to play a vital role in Indonesian coal deep-processing.'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.run(\"Who are the main exporters of Coal to China? What is the role of Indonesia in this?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b31086b",
   "metadata": {},
   "source": [
    "The output of `qa_chain.run()` provides the generated answer based on the given prompts or questions using the question-answering system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4f3ce046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" This document provides information about the impacts of the global energy crisis and the Indonesian coal ban on coal and gas exports from Australia, China, and Indonesia. Australia's coal and gas exports are projected to decrease by half in the next five years due to the passing of its peak and efforts to reduce greenhouse gas emissions. In China, coal imports from Russia have risen and Mongolia is set to be the largest coking coal seller to China for the third consecutive year. Meanwhile, Indonesia is set to produce a record 695 million tonnes of coal in 2023 and export 518 million tonnes. Ford is investing in a nickel processing facility in Indonesia and the country's economy is expected to experience strong growth in 2023.\""
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.run(\"please give me summary of this document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780e00d",
   "metadata": {},
   "source": [
    "# HuggingFace\n",
    "\n",
    "Hugging Face is a company that specializes in natural language processing (NLP) and provides tools and models to help with NLP tasks. \n",
    "\n",
    "One of their popular offerings is the **\"Transformers\"**\" library, which offers **\"pre-trained**\" models for various NLP tasks like text classification, sentiment analysis, and question answering. These models are based on advanced architectures like GPT (Generative Pre-trained Transformer) developed by OpenAI. \n",
    "\n",
    "Hugging Face also has a platform called the **\"\"Hugging Face Hub\"**\" where users can **\"access and share models**\" and datasets. It simplifies the process of using pre-trained models and promotes collaboration among researchers and developers. Langchain allows us to connect with the Hugging Face API, giving us access to additional Transformers models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a813d2c",
   "metadata": {},
   "source": [
    "## Setting up API Key and `.env` file\n",
    "\n",
    "#### `.env` file\n",
    "\n",
    "To use the Hugging Face API, we need to create an API Key. You can create your API Key by going to [this link](https://huggingface.co/settings/tokens). Once we have the API Key, we need to store it in a file called `.env`.\n",
    "\n",
    "To create the `.env` file, open a text editor and enter the following information:\n",
    "\n",
    "```plaintext\n",
    "OPENAI_API_KEY={your_openai_api_key}\n",
    "HUGGINGFACEHUB_API_TOKEN={your_huggingface_api_key}\n",
    "```\n",
    "\n",
    "Replace `{your_openai_api_key}` and `{your_huggingface_api_key}` with the respective API keys.\n",
    "\n",
    "Save the file as `.env` in the same directory as your Python script or notebook.\n",
    "\n",
    "To load the environment variables from the `.env` file, we can use the `load_dotenv()` function from the `dotenv` library. Make sure to add the following line of code at the beginning of your script or notebook:\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "083bcc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c077628",
   "metadata": {},
   "source": [
    "By doing this, Python will recognize and use the API keys stored in the `.env` file throughout your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2808692b",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce54a28",
   "metadata": {},
   "source": [
    "To implement Hugging Face models and utilize the Hugging Face Hub in Langchain, we need to import the following modules:\n",
    "- `HuggingFaceHub` to access and interact with the Hugging Face models and datasets from the hub.\n",
    "- `LLMChain` to create a chain using a Hugging Face language model for text generation and processing.\n",
    "- `PromptTemplate` to create customizable prompt templates for generating prompts based on user inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f61236cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac552fd3",
   "metadata": {},
   "source": [
    "Similar to the OpenAI integration, when working with Hugging Face models in Langchain, we also need to specify the desired model to use. This involves selecting a pre-trained model from the Hugging Face Hub based on our task or application.\n",
    "\n",
    "Once the model is chosen, we can set various parameters specific to the Hugging Face model, such as:\n",
    "\n",
    "- `temperature` parameter to control the randomness of the generated text,\n",
    "- `max_length` parameter to limit the length of the generated output.\n",
    "\n",
    "These parameters allow us to **fine-tune** the behavior of the Hugging Face model and customize its responses to align with our specific requirements. By leveraging these settings, we can optimize the model's output and achieve the desired results in our text generation and processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "89a8dbba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hub_llm = HuggingFaceHub(\n",
    "    repo_id='gpt2',\n",
    "    model_kwargs={'temperature': 0, 'max_length': 50}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060bdf0",
   "metadata": {},
   "source": [
    "To create a chain using Hugging Face models in Langchain, we need to define a prompt template that specifies the desired format of the input. \n",
    "\n",
    "This prompt template acts as a guideline for structuring the input data that will be passed to the model. Once the prompt template is defined, we can create the chain by initializing an instance of the LLMChain class. This chain connects the prompt template and the Hugging Face model, allowing us to generate responses based on the provided input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c957eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"Question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "hub_chain = LLMChain(prompt=prompt, llm=hub_llm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb13e660",
   "metadata": {},
   "source": [
    "Let's ask some interesting questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b1268d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: who won FIFA World Cup in the year 1994?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nA: The winner of the World Cup in 1994 was the United States.\\n\\nQ: What was the most important thing about the World Cup?\\n\\nA: The World'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub_chain.run(\"who won FIFA World Cup in the year 1994?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cf4915",
   "metadata": {},
   "source": [
    "The output from the code is the response generated by the language model based on the given prompt. However, in this case, the output may not make sense or provide the correct answer because the model used (`hub_llm`) may not have been specifically trained to answer questions about the \"FIFA World Cup in the year 1994\". \n",
    "\n",
    "The language model's response is based on the patterns it has learned from the training data, and if the specific information is not present in the training data, the model may generate a response that is not accurate or relevant to the question. It's important to note that the effectiveness of the response depends on the training data and the capabilities of the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca02f22",
   "metadata": {},
   "source": [
    "## Integrating HuggingFace's Inference API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f45402",
   "metadata": {},
   "source": [
    "What if we want to use a free model from Hugging Face to create a question answering system based on our database? We can import a pre-trained model that can translate natural language queries into SQL queries. One of the models available is `t5-base-finetuned-wikiSQL`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d34c27",
   "metadata": {},
   "source": [
    "To integrate the model, we need to create an `HuggingFaceHub` object. We can set the `repo_id` parameter to specify the repository ID of the desired model, and the `model_kwargs` to configure the model. For example, we can set `temperature=0` to get deterministic responses. This allows us to retrieve the model from the Hugging Face Hub and use it for our question answering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8a5aa45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the llm model from huggingface\n",
    "hf_llm_t5 = HuggingFaceHub(\n",
    "    repo_id='mrm8488/t5-base-finetuned-wikiSQL',\n",
    "    model_kwargs={'temperature': 0, 'max_token':2000}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be4082",
   "metadata": {},
   "source": [
    "After importing the necessary modules and loading the model, we can create a prompt template using `PromptTemplate`. \n",
    "\n",
    "- The `input_variables` parameter specifies the input variables we want to use, \n",
    "- The `template` parameter defines the template string with the placeholders for the input variables. \n",
    "\n",
    "This allows us to easily customize the prompt based on our specific question answering needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "de6e72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt template\n",
    "prompt_db = PromptTemplate(\n",
    "    input_variables=['question'],\n",
    "    template=\"Translate English to SQL: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c19d5",
   "metadata": {},
   "source": [
    "After setting up the prompt template and model, we can chain all the components together using `LLMChain`. This allows us to connect the prompt template with the language model and create a unified system for question answering. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fd893faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chaining\n",
    "hub_chain = LLMChain(prompt=prompt_db, llm=hf_llm_t5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabfcbc0",
   "metadata": {},
   "source": [
    "\n",
    "Once the chain is established, we can use the `run()` method to generate responses based on the input provided through the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8afb483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTranslate English to SQL: How many rows is in the tracks's table?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SELECT COUNT Rows FROM table WHERE Tracks = Tracks'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the template\n",
    "hub_chain.run(\"How many rows is in the tracks's table?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b22e6",
   "metadata": {},
   "source": [
    "The incorrect answer may be due to a difference in syntax between the T5 model and the OpenAI model. In the T5 model, the query should include `FROM tracks` instead of just `table`. This highlights the importance of using the correct syntax for the specific model being used.\n",
    "\n",
    "It's worth noting that HuggingFace provides a wide range of models that are designed for specific tasks. These models have been fine-tuned on specific datasets and can be a valuable resource for various natural language processing tasks, including question answering. By selecting the appropriate model for the task at hand, we can improve the accuracy and reliability of the answers generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932a5fb",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this module, we explored various aspects of working with large language models (LLMs) and building question-answering systems. We started by understanding the architecture and key concepts of large language models, including the Transformer model and the process of pre-training and fine-tuning. We also introduced popular LLMs like GPT-3, GPT-2, and BERT, discussing their capabilities and limitations. We then delved into the LangChain concept, which allows us to connect databases and text data with LLMs for question answering. We demonstrated the use of OpenAI and LangChain to build question-answering systems with both databases and text data. Additionally, we explored text generation using HuggingFace's models, covering the setup and integration of the HuggingFace Hub. Overall, this module provided insights into harnessing the power of large language models for question answering and text generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5449f91a",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238.927px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
